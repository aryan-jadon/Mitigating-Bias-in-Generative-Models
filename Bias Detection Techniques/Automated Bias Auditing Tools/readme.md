## Automated Bias Auditing Tools

1. [AI Fairness 360 (AIF360)](#ai-fairness-360-aif360)
2. [Fairness Indicators](#fairness-indicators)
3. [Fairlearn](#fairlearn)
4. [Themis-ML](#themis-ml)
5. [What-If Tool (WIT)](#what-if-tool-wit)
6. [Fairness Comparison](#fairness-comparison)
7. [Fairness360](#fairness360)
8. [Fair-SMOTE](#fair-smote)

### 1. AI Fairness 360 (AIF360)

AI Fairness 360 (AIF360) is an open-source library developed by IBM to help detect and mitigate bias in machine learning models. 
It provides a comprehensive set of metrics and algorithms to test for and address biases, making it easier for developers to create fairer models.

**Project GitHub Link :**[AIF360](https://github.com/Trusted-AI/AIF360)

### 2. Fairness Indicators

Fairness Indicators is a suite of tools built on TensorFlow for evaluating fairness in machine learning models. It includes metrics and visualizations to help understand model performance across different slices of data and ensure equitable outcomes.

**GitHub:** [Fairness Indicators](https://github.com/tensorflow/fairness-indicators)

### 3. Fairlearn

Fairlearn is a Python package to assess and improve the fairness of machine learning models. It offers a variety of algorithms for mitigating bias, along with tools for evaluating and visualizing fairness metrics.

**GitHub:** [Fairlearn](https://github.com/fairlearn/fairlearn)

### 4. Themis-ML

Themis-ML is a fairness-aware machine learning library designed to detect and mitigate discrimination in predictive modeling. It provides tools for bias detection and a range of algorithms to ensure fair model predictions.

**GitHub:** [Themis-ML](https://github.com/cosmicBboy/themis-ml)

### 5. What-If Tool (WIT)

The What-If Tool (WIT) is a visualization tool built by Google PAIR for analyzing machine learning models and datasets. It focuses on understanding model performance and fairness by providing interactive exploration of model behavior across different data slices.

**GitHub:** [What-If Tool (WIT)](https://github.com/PAIR-code/what-if-tool)

### 6. Fairness Comparison

Fairness Comparison is a library designed for comparing different fairness metrics and bias mitigation algorithms. It helps researchers and practitioners to systematically evaluate the effectiveness of various fairness interventions.

**GitHub:**
[Fairness Comparison](https://github.com/algofairness/fairness-comparison)

### 7. Fairness360

Fairness360 is a toolkit developed by DeepMind to help machine learning developers measure and understand the fairness of their models. It includes tools for assessing bias and evaluating the impact of fairness interventions.

**GitHub:** [Fairness360](https://github.com/deepmind/fairness)

### 8. Fair-SMOTE

Fair-SMOTE is a tool for generating synthetic data to balance class distribution while preserving fairness. It extends the popular SMOTE algorithm with fairness constraints, ensuring that the synthetic data does not introduce additional bias.

**GitHub:** [Fair-SMOTE](https://github.com/marcoscastro/fair-smote)
